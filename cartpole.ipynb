{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison d'algorithmes de Deep RL\n",
    "\n",
    "Dans la pratique de nombreux problèmes font intervenir une quantité gigantesques d'états possibles. Une diversité d'états rendent l'apprentissage de la table Q ou V particulièrement complexe, voir impossible pour une échelle de temps humaine (vis à vis des capacités informatiques actuelles). \n",
    "\n",
    "Des prémices de ce problème peuvent être observés en comparant le temps d'apprentissage de Q pour l'environnement \"Frozen Lake 4x4\" et \"Fronze Lake 8x8\". Dans la première situation l'entraînement (de Q) ne demandait que quelques secondes et convergeait vers la solution optimale. Dans le second cas il a fallu plus de 30 minutes d'entraînement (sur le même ordinateur) pour atteindre la solution optimale, cela alors que le nombre d'état n'a été multiplé que par 4 ! (De 16 à 64).\n",
    "\n",
    "\n",
    "Une approche consiste à remplacer la table Q par un réseau de neurones profond (d'où le nom). On considère alors les décisions du réseaux étant donné un état comme une politique et à utiliser le fait que l'opérateur des équations de Bellman optimales est une contraction pour assurer la convergence de notre algo. (On compare les choix de l'algorithme à ceux qu'il aurait fait en suivant sa propre politique gourmande pour l'entraîner).\n",
    "\n",
    "Ce notebook illustre les performances de deux algorithmes de Deep RL : Reinforce et DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erwan/miniconda3/envs/torch-env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gym \n",
    "import torch\n",
    "\n",
    "from tool_functions import evaluate_drl_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32) états possibles\n",
      "Il y a Discrete(2) actions possibles\n"
     ]
    }
   ],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "env = gym.make(env_id)\n",
    "\n",
    "state_space, s_size = env.observation_space, env.observation_space.shape[0]\n",
    "print(\"Il y a\", state_space, \"états possibles\")\n",
    "\n",
    "action_space, a_size = env.action_space, env.action_space.n\n",
    "print(\"Il y a\", action_space, \"actions possibles\")\n",
    "\n",
    "# Environnement d'évaluation\n",
    "eval_env = gym.make(env_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beaucoup trop d'états possibles pour initialiser une table Q -> l'utilisation des algorithmes avec les tables est trop complexe pour \n",
    "donner des bonnes performances ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole_hyperparameters = {\n",
    "    \"h_size\": 16,\n",
    "    \"n_training_episodes\": 50,\n",
    "    \"max_t\": 1000,\n",
    "    \"gamma\": 1.0,\n",
    "    \"lr\": 1e-2,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithme Reinforce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Deep_RL.reinforce import Policy, reinforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage Score: 18.10, Standard Deviation Score: 5.41\n",
      "Métrique : 12.69\n",
      "\t Nouveau meilleur score! Sauvegarde des poids du modèle à /Users/erwan/Programmes/M2DS RL/RL M2DS/best_models/reinforce_MC_CartPole-v1_12.69\n",
      "Terminé !\n"
     ]
    }
   ],
   "source": [
    "reinforce_policy =  Policy(\n",
    "    game_name=cartpole_hyperparameters['env_id'],\n",
    "    s_size=cartpole_hyperparameters['state_space'],\n",
    "    a_size=cartpole_hyperparameters['action_space'],\n",
    "    h_size=cartpole_hyperparameters['h_size']   \n",
    ")\n",
    "\n",
    "reinforce_optimizer = torch.optim.Adam(reinforce_policy.parameters(), lr=cartpole_hyperparameters['lr'])\n",
    "\n",
    "# Entraînement\n",
    "policy, scores = reinforce(reinforce_policy,\n",
    "                   reinforce_optimizer,\n",
    "                   cartpole_hyperparameters[\"n_training_episodes\"], \n",
    "                   cartpole_hyperparameters[\"max_t\"],\n",
    "                   cartpole_hyperparameters[\"gamma\"], \n",
    "                   env=env,\n",
    "                   check_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 89.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 34.08, Std : 19.068654907989707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mean_rwd, std_rwd = evaluate_drl_agent(\n",
    "    eval_env, \n",
    "    max_steps=1000, \n",
    "    n_eval_episodes=100,\n",
    "    model=policy)\n",
    "\n",
    "print('Mean reward: {}, Std : {}'.format(mean_rwd, std_rwd))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithme DQN\n",
    "\n",
    "On utilise l'implémentation de Stable Baseline 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7f87d37ab670>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn_model = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "dqn_model.learn(total_timesteps=cartpole_hyperparameters['n_training_episodes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 329.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 11.06, Std : 1.4684685900624503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mean_rwd, std_rwd = evaluate_drl_agent(\n",
    "    eval_env, \n",
    "    max_steps=1000, \n",
    "    n_eval_episodes=100,\n",
    "    model=dqn_model)\n",
    "\n",
    "print('Mean reward: {}, Std : {}'.format(mean_rwd, std_rwd))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0acbd71ee6978b2565a23648ddbf4a93f58fd633182475feae718b7ed62347c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
