{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison d'algorithmes de Deep RL\n",
    "\n",
    "Dans la pratique de nombreux problèmes font intervenir une quantité gigantesques d'états possibles. Une diversité d'états rendent l'apprentissage de la table Q ou V particulièrement complexe, voir impossible pour une échelle de temps humaine (vis à vis des capacités informatiques actuelles). \n",
    "\n",
    "Des prémices de ce problème peuvent être observées en comparant le temps d'apprentissage de Q pour l'environnement \"Frozen Lake 4x4\" et celui de \"Frozen Lake 8x8\". Dans la première situation l'entraînement (de Q) ne demandait que quelques secondes et convergeait vers la solution optimale. Dans le second cas il a fallu plus de 30 minutes d'entraînement (sur le même ordinateur) pour atteindre la solution optimale, cela alors que le nombre d'états n'a été multiplé que par 4 ! (De 16 à 64).\n",
    "\n",
    "\n",
    "Une approche consiste à remplacer la table Q par un réseau de neurones profond (d'où le nom \"DeepRL\"). On considère alors les décisions du réseaux (sortie du réseau) étant donné un état (entrée du réseau) comme une politique et on utilise le fait que l'opérateur des équations de Bellman optimales est une contraction pour assurer la convergence de notre algo. (Pour une trajectoire donnée compare les choix de l'algorithme à ceux qu'il aurait fait en suivant sa propre politique gourmande pour l'entraîner).\n",
    "\n",
    "Ce notebook illustre les performances de deux algorithmes de Deep RL : Reinforce et DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import torch\n",
    "\n",
    "from tool_functions import evaluate_drl_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32) états possibles\n",
      "Il y a Discrete(2) actions possibles\n"
     ]
    }
   ],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "env = gym.make(env_id)\n",
    "\n",
    "state_space, s_size = env.observation_space, env.observation_space.shape[0]\n",
    "print(\"Il y a\", state_space, \"états possibles\")\n",
    "\n",
    "action_space, a_size = env.action_space, env.action_space.n\n",
    "print(\"Il y a\", action_space, \"actions possibles\")\n",
    "\n",
    "# Environnement d'évaluation\n",
    "eval_env = gym.make(env_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beaucoup trop d'états possibles pour initialiser une table Q -> l'utilisation des algorithmes avec les tables est trop complexe pour \n",
    "donner des bonnes performances ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole_hyperparameters = {\n",
    "    \"h_size\": 16,\n",
    "    \"n_training_episodes\": 1000,\n",
    "    \"max_t\": 1000,\n",
    "    \"gamma\": 1.0,\n",
    "    \"lr\": 1e-2,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithme Reinforce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Deep_RL.reinforce import Policy, reinforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce_policy =  Policy(\n",
    "    game_name=cartpole_hyperparameters['env_id'],\n",
    "    s_size=cartpole_hyperparameters['state_space'],\n",
    "    a_size=cartpole_hyperparameters['action_space'],\n",
    "    h_size=cartpole_hyperparameters['h_size']   \n",
    ")\n",
    "\n",
    "reinforce_optimizer = torch.optim.Adam(reinforce_policy.parameters(), lr=cartpole_hyperparameters['lr'])\n",
    "\n",
    "# Entraînement\n",
    "policy, scores = reinforce(reinforce_policy,\n",
    "                   reinforce_optimizer,\n",
    "                   cartpole_hyperparameters[\"n_training_episodes\"], \n",
    "                   cartpole_hyperparameters[\"max_t\"],\n",
    "                   cartpole_hyperparameters[\"gamma\"], \n",
    "                   env=env,\n",
    "                   check_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:14<00:00,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 499.56, Std : 4.377944723269128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mean_rwd, std_rwd = evaluate_drl_agent(\n",
    "    eval_env, \n",
    "    max_steps=1000, \n",
    "    n_eval_episodes=100,\n",
    "    model=policy)\n",
    "\n",
    "print('Mean reward: {}, Std : {}'.format(mean_rwd, std_rwd))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithme DQN\n",
    "\n",
    "On utilise l'implémentation de Stable Baseline 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_model = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "dqn_model.learn(total_timesteps=cartpole_hyperparameters['n_training_episodes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 321.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 9.49, Std : 0.9326842981416594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mean_rwd, std_rwd = evaluate_drl_agent(\n",
    "    eval_env, \n",
    "    max_steps=1000, \n",
    "    n_eval_episodes=100,\n",
    "    model=dqn_model)\n",
    "\n",
    "print('Mean reward: {}, Std : {}'.format(mean_rwd, std_rwd))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Le modèles DQN, contrairement à celui de Reinforce, nécessite beaucoup plus d'itérations, d'où les résultats si différents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer_training_dqn_model = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "longer_training_dqn_model.learn(total_timesteps=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 18.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 226.05, Std : 25.166793597913898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mean_rwd, std_rwd = evaluate_drl_agent(\n",
    "    eval_env, \n",
    "    max_steps=1000, \n",
    "    n_eval_episodes=100,\n",
    "    model=longer_training_dqn_model)\n",
    "\n",
    "print('Mean reward: {}, Std : {}'.format(mean_rwd, std_rwd))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Même malgré 100_000 itérations les résultats restent bien inférieurs, pour atteindre 500.0 de score il faudrait au moins 1_000_000 d'itérations (réalisé sur colab avec GPU)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0acbd71ee6978b2565a23648ddbf4a93f58fd633182475feae718b7ed62347c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
